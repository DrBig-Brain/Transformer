{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9aac053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  English  Spanish                                        Attribution\n",
      "0     Go.      Ve.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
      "1     Go.    Vete.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
      "2     Go.    Vaya.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
      "3     Go.  V\u00e1yase.  CC-BY 2.0 (France) Attribution: tatoeba.org #2...\n",
      "4     Hi.    Hola.  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"spa-eng/spa.txt\", sep=\"\\t\", header=None, names=[\"English\", \"Spanish\", \"Attribution\"], encoding=\"utf-8\")\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6582dfaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(139013, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0542cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_eng = []\n",
    "for i in df['English']:\n",
    "    l_eng.append(len(i.split(\" \")))\n",
    "max(l_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5622591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_spa = []\n",
    "for i in df['Spanish']:\n",
    "    l_spa.append(len(i.split(\" \")))\n",
    "max(l_spa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "318b8da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "en_sent = df['English']\n",
    "sp_sent = df['Spanish']\n",
    "en_sent = en_sent.apply(lambda x: x.lower())\n",
    "sp_sent = sp_sent.apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d81eb7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_sent = en_sent.apply(lambda x: re.sub(r'[,.!?\u00bf\u00a1]', '', x))\n",
    "sp_sent = sp_sent.apply(lambda x: re.sub(r'[,.!?\u00bf\u00a1]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671384e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_sent = sp_sent.apply(lambda x: '<sos>' + x + '<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f4debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder import Encoder\n",
    "en_encoder = Encoder(en_sent,max_len = 100)\n",
    "sp_encoder = Encoder(sp_sent,max_len = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7217760c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_encoder.build_vocab()\n",
    "len(en_encoder.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02c3fd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_encoder.build_vocab()\n",
    "len(sp_encoder.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f09f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_encoded = []\n",
    "for i in en_sent.values:\n",
    "    en_encoded.append(en_encoder.encode_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05ed749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_encoded = []\n",
    "for i in sp_sent.values:\n",
    "    sp_encoded.append(sp_encoder.encode_text(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02f1775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "from model import Transformer\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(10000,10000,128,2,6,2048,100,0.1).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "302e0c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress:0 / 139013\n",
      "progress:128 / 139013\n",
      "progress:256 / 139013\n",
      "progress:384 / 139013\n",
      "progress:512 / 139013\n",
      "progress:640 / 139013\n",
      "progress:768 / 139013\n",
      "progress:896 / 139013\n",
      "progress:1024 / 139013\n",
      "progress:1152 / 139013\n",
      "progress:1280 / 139013\n",
      "progress:1408 / 139013\n",
      "progress:1536 / 139013\n",
      "progress:1664 / 139013\n",
      "progress:1792 / 139013\n",
      "progress:1920 / 139013\n",
      "progress:2048 / 139013\n",
      "progress:2176 / 139013\n",
      "progress:2304 / 139013\n",
      "progress:2432 / 139013\n",
      "progress:2560 / 139013\n",
      "progress:2688 / 139013\n",
      "progress:2816 / 139013\n",
      "progress:2944 / 139013\n",
      "progress:3072 / 139013\n",
      "progress:3200 / 139013\n",
      "progress:3328 / 139013\n",
      "progress:3456 / 139013\n",
      "progress:3584 / 139013\n",
      "progress:3712 / 139013\n",
      "progress:3840 / 139013\n",
      "progress:3968 / 139013\n",
      "progress:4096 / 139013\n",
      "progress:4224 / 139013\n",
      "progress:4352 / 139013\n",
      "progress:4480 / 139013\n",
      "progress:4608 / 139013\n",
      "progress:4736 / 139013\n",
      "progress:4864 / 139013\n",
      "progress:4992 / 139013\n",
      "progress:5120 / 139013\n",
      "progress:5248 / 139013\n",
      "progress:5376 / 139013\n",
      "progress:5504 / 139013\n",
      "progress:5632 / 139013\n",
      "progress:5760 / 139013\n",
      "progress:5888 / 139013\n",
      "progress:6016 / 139013\n",
      "progress:6144 / 139013\n",
      "progress:6272 / 139013\n",
      "progress:6400 / 139013\n",
      "progress:6528 / 139013\n",
      "progress:6656 / 139013\n",
      "progress:6784 / 139013\n",
      "progress:6912 / 139013\n",
      "progress:7040 / 139013\n",
      "progress:7168 / 139013\n",
      "progress:7296 / 139013\n",
      "progress:7424 / 139013\n",
      "progress:7552 / 139013\n",
      "progress:7680 / 139013\n",
      "progress:7808 / 139013\n",
      "progress:7936 / 139013\n",
      "progress:8064 / 139013\n",
      "progress:8192 / 139013\n",
      "progress:8320 / 139013\n",
      "progress:8448 / 139013\n",
      "progress:8576 / 139013\n",
      "progress:8704 / 139013\n",
      "progress:8832 / 139013\n",
      "progress:8960 / 139013\n",
      "progress:9088 / 139013\n",
      "progress:9216 / 139013\n",
      "progress:9344 / 139013\n",
      "progress:9472 / 139013\n",
      "progress:9600 / 139013\n",
      "progress:9728 / 139013\n",
      "progress:9856 / 139013\n",
      "progress:9984 / 139013\n",
      "progress:10112 / 139013\n",
      "progress:10240 / 139013\n",
      "progress:10368 / 139013\n",
      "progress:10496 / 139013\n",
      "progress:10624 / 139013\n",
      "progress:10752 / 139013\n",
      "progress:10880 / 139013\n",
      "progress:11008 / 139013\n",
      "progress:11136 / 139013\n",
      "progress:11264 / 139013\n",
      "progress:11392 / 139013\n",
      "progress:11520 / 139013\n",
      "progress:11648 / 139013\n",
      "progress:11776 / 139013\n",
      "progress:11904 / 139013\n",
      "progress:12032 / 139013\n",
      "progress:12160 / 139013\n",
      "progress:12288 / 139013\n",
      "progress:12416 / 139013\n",
      "progress:12544 / 139013\n",
      "progress:12672 / 139013\n",
      "progress:12800 / 139013\n",
      "progress:12928 / 139013\n",
      "progress:13056 / 139013\n",
      "progress:13184 / 139013\n",
      "progress:13312 / 139013\n",
      "progress:13440 / 139013\n",
      "progress:13568 / 139013\n",
      "progress:13696 / 139013\n",
      "progress:13824 / 139013\n",
      "progress:13952 / 139013\n",
      "progress:14080 / 139013\n",
      "progress:14208 / 139013\n",
      "progress:14336 / 139013\n",
      "progress:14464 / 139013\n",
      "progress:14592 / 139013\n",
      "progress:14720 / 139013\n",
      "progress:14848 / 139013\n",
      "progress:14976 / 139013\n",
      "progress:15104 / 139013\n",
      "progress:15232 / 139013\n",
      "progress:15360 / 139013\n",
      "progress:15488 / 139013\n",
      "progress:15616 / 139013\n",
      "progress:15744 / 139013\n",
      "progress:15872 / 139013\n",
      "progress:16000 / 139013\n",
      "progress:16128 / 139013\n",
      "progress:16256 / 139013\n",
      "progress:16384 / 139013\n",
      "progress:16512 / 139013\n",
      "progress:16640 / 139013\n",
      "progress:16768 / 139013\n",
      "progress:16896 / 139013\n",
      "progress:17024 / 139013\n",
      "progress:17152 / 139013\n",
      "progress:17280 / 139013\n",
      "progress:17408 / 139013\n",
      "progress:17536 / 139013\n",
      "progress:17664 / 139013\n",
      "progress:17792 / 139013\n",
      "progress:17920 / 139013\n",
      "progress:18048 / 139013\n",
      "progress:18176 / 139013\n",
      "progress:18304 / 139013\n",
      "progress:18432 / 139013\n",
      "progress:18560 / 139013\n",
      "progress:18688 / 139013\n",
      "progress:18816 / 139013\n",
      "progress:18944 / 139013\n",
      "progress:19072 / 139013\n",
      "progress:19200 / 139013\n",
      "progress:19328 / 139013\n",
      "progress:19456 / 139013\n",
      "progress:19584 / 139013\n",
      "progress:19712 / 139013\n",
      "progress:19840 / 139013\n",
      "progress:19968 / 139013\n",
      "progress:20096 / 139013\n",
      "progress:20224 / 139013\n",
      "progress:20352 / 139013\n",
      "progress:20480 / 139013\n",
      "progress:20608 / 139013\n",
      "progress:20736 / 139013\n",
      "progress:20864 / 139013\n",
      "progress:20992 / 139013\n",
      "progress:21120 / 139013\n",
      "progress:21248 / 139013\n",
      "progress:21376 / 139013\n",
      "progress:21504 / 139013\n",
      "progress:21632 / 139013\n",
      "progress:21760 / 139013\n",
      "progress:21888 / 139013\n",
      "progress:22016 / 139013\n",
      "progress:22144 / 139013\n",
      "progress:22272 / 139013\n",
      "progress:22400 / 139013\n",
      "progress:22528 / 139013\n",
      "progress:22656 / 139013\n",
      "progress:22784 / 139013\n",
      "progress:22912 / 139013\n",
      "progress:23040 / 139013\n",
      "progress:23168 / 139013\n",
      "progress:23296 / 139013\n",
      "progress:23424 / 139013\n",
      "progress:23552 / 139013\n",
      "progress:23680 / 139013\n",
      "progress:23808 / 139013\n",
      "progress:23936 / 139013\n",
      "progress:24064 / 139013\n",
      "progress:24192 / 139013\n",
      "progress:24320 / 139013\n",
      "progress:24448 / 139013\n",
      "progress:24576 / 139013\n",
      "progress:24704 / 139013\n",
      "progress:24832 / 139013\n",
      "progress:24960 / 139013\n",
      "progress:25088 / 139013\n",
      "progress:25216 / 139013\n",
      "progress:25344 / 139013\n",
      "progress:25472 / 139013\n",
      "progress:25600 / 139013\n",
      "progress:25728 / 139013\n",
      "progress:25856 / 139013\n",
      "progress:25984 / 139013\n",
      "progress:26112 / 139013\n",
      "progress:26240 / 139013\n",
      "progress:26368 / 139013\n",
      "progress:26496 / 139013\n",
      "progress:26624 / 139013\n",
      "progress:26752 / 139013\n",
      "progress:26880 / 139013\n",
      "progress:27008 / 139013\n",
      "progress:27136 / 139013\n",
      "progress:27264 / 139013\n",
      "progress:27392 / 139013\n",
      "progress:27520 / 139013\n",
      "progress:27648 / 139013\n",
      "progress:27776 / 139013\n",
      "progress:27904 / 139013\n",
      "progress:28032 / 139013\n",
      "progress:28160 / 139013\n",
      "progress:28288 / 139013\n",
      "progress:28416 / 139013\n",
      "progress:28544 / 139013\n",
      "progress:28672 / 139013\n",
      "progress:28800 / 139013\n",
      "progress:28928 / 139013\n",
      "progress:29056 / 139013\n",
      "progress:29184 / 139013\n",
      "progress:29312 / 139013\n",
      "progress:29440 / 139013\n",
      "progress:29568 / 139013\n",
      "progress:29696 / 139013\n",
      "progress:29824 / 139013\n",
      "progress:29952 / 139013\n",
      "progress:30080 / 139013\n",
      "progress:30208 / 139013\n",
      "progress:30336 / 139013\n",
      "progress:30464 / 139013\n",
      "progress:30592 / 139013\n",
      "progress:30720 / 139013\n",
      "progress:30848 / 139013\n",
      "progress:30976 / 139013\n",
      "progress:31104 / 139013\n",
      "progress:31232 / 139013\n",
      "progress:31360 / 139013\n",
      "progress:31488 / 139013\n",
      "progress:31616 / 139013\n",
      "progress:31744 / 139013\n",
      "progress:31872 / 139013\n",
      "progress:32000 / 139013\n",
      "progress:32128 / 139013\n",
      "progress:32256 / 139013\n",
      "progress:32384 / 139013\n",
      "progress:32512 / 139013\n",
      "progress:32640 / 139013\n",
      "progress:32768 / 139013\n",
      "progress:32896 / 139013\n",
      "progress:33024 / 139013\n",
      "progress:33152 / 139013\n",
      "progress:33280 / 139013\n",
      "progress:33408 / 139013\n",
      "progress:33536 / 139013\n",
      "progress:33664 / 139013\n",
      "progress:33792 / 139013\n",
      "progress:33920 / 139013\n",
      "progress:34048 / 139013\n",
      "progress:34176 / 139013\n",
      "progress:34304 / 139013\n",
      "progress:34432 / 139013\n",
      "progress:34560 / 139013\n",
      "progress:34688 / 139013\n",
      "progress:34816 / 139013\n",
      "progress:34944 / 139013\n",
      "progress:35072 / 139013\n",
      "progress:35200 / 139013\n",
      "progress:35328 / 139013\n",
      "progress:35456 / 139013\n",
      "progress:35584 / 139013\n",
      "progress:35712 / 139013\n",
      "progress:35840 / 139013\n",
      "progress:35968 / 139013\n",
      "progress:36096 / 139013\n",
      "progress:36224 / 139013\n",
      "progress:36352 / 139013\n",
      "progress:36480 / 139013\n",
      "progress:36608 / 139013\n",
      "progress:36736 / 139013\n",
      "progress:36864 / 139013\n",
      "progress:36992 / 139013\n",
      "progress:37120 / 139013\n",
      "progress:37248 / 139013\n",
      "progress:37376 / 139013\n",
      "progress:37504 / 139013\n",
      "progress:37632 / 139013\n",
      "progress:37760 / 139013\n",
      "progress:37888 / 139013\n",
      "progress:38016 / 139013\n",
      "progress:38144 / 139013\n",
      "progress:38272 / 139013\n",
      "progress:38400 / 139013\n",
      "progress:38528 / 139013\n",
      "progress:38656 / 139013\n",
      "progress:38784 / 139013\n",
      "progress:38912 / 139013\n",
      "progress:39040 / 139013\n",
      "progress:39168 / 139013\n",
      "progress:39296 / 139013\n",
      "progress:39424 / 139013\n",
      "progress:39552 / 139013\n",
      "progress:39680 / 139013\n",
      "progress:39808 / 139013\n",
      "progress:39936 / 139013\n",
      "progress:40064 / 139013\n",
      "progress:40192 / 139013\n",
      "progress:40320 / 139013\n",
      "progress:40448 / 139013\n",
      "progress:40576 / 139013\n",
      "progress:40704 / 139013\n",
      "progress:40832 / 139013\n",
      "progress:40960 / 139013\n",
      "progress:41088 / 139013\n",
      "progress:41216 / 139013\n",
      "progress:41344 / 139013\n",
      "progress:41472 / 139013\n",
      "progress:41600 / 139013\n",
      "progress:41728 / 139013\n",
      "progress:41856 / 139013\n",
      "progress:41984 / 139013\n",
      "progress:42112 / 139013\n",
      "progress:42240 / 139013\n",
      "progress:42368 / 139013\n",
      "progress:42496 / 139013\n",
      "progress:42624 / 139013\n",
      "progress:42752 / 139013\n",
      "progress:42880 / 139013\n",
      "progress:43008 / 139013\n",
      "progress:43136 / 139013\n",
      "progress:43264 / 139013\n",
      "progress:43392 / 139013\n",
      "progress:43520 / 139013\n",
      "progress:43648 / 139013\n",
      "progress:43776 / 139013\n",
      "progress:43904 / 139013\n",
      "progress:44032 / 139013\n",
      "progress:44160 / 139013\n",
      "progress:44288 / 139013\n",
      "progress:44416 / 139013\n",
      "progress:44544 / 139013\n",
      "progress:44672 / 139013\n",
      "progress:44800 / 139013\n",
      "progress:44928 / 139013\n",
      "progress:45056 / 139013\n",
      "progress:45184 / 139013\n",
      "progress:45312 / 139013\n",
      "progress:45440 / 139013\n",
      "progress:45568 / 139013\n",
      "progress:45696 / 139013\n",
      "progress:45824 / 139013\n",
      "progress:45952 / 139013\n",
      "progress:46080 / 139013\n",
      "progress:46208 / 139013\n",
      "progress:46336 / 139013\n",
      "progress:46464 / 139013\n",
      "progress:46592 / 139013\n",
      "progress:46720 / 139013\n",
      "progress:46848 / 139013\n",
      "progress:46976 / 139013\n",
      "progress:47104 / 139013\n",
      "progress:47232 / 139013\n",
      "progress:47360 / 139013\n",
      "progress:47488 / 139013\n",
      "progress:47616 / 139013\n",
      "progress:47744 / 139013\n",
      "progress:47872 / 139013\n",
      "progress:48000 / 139013\n",
      "progress:48128 / 139013\n",
      "progress:48256 / 139013\n",
      "progress:48384 / 139013\n",
      "progress:48512 / 139013\n",
      "progress:48640 / 139013\n",
      "progress:48768 / 139013\n",
      "progress:48896 / 139013\n",
      "progress:49024 / 139013\n",
      "progress:49152 / 139013\n",
      "progress:49280 / 139013\n",
      "progress:49408 / 139013\n",
      "progress:49536 / 139013\n",
      "progress:49664 / 139013\n",
      "progress:49792 / 139013\n",
      "progress:49920 / 139013\n",
      "progress:50048 / 139013\n",
      "progress:50176 / 139013\n",
      "progress:50304 / 139013\n",
      "progress:50432 / 139013\n",
      "progress:50560 / 139013\n",
      "progress:50688 / 139013\n",
      "progress:50816 / 139013\n",
      "progress:50944 / 139013\n",
      "progress:51072 / 139013\n",
      "progress:51200 / 139013\n",
      "progress:51328 / 139013\n",
      "progress:51456 / 139013\n",
      "progress:51584 / 139013\n",
      "progress:51712 / 139013\n",
      "progress:51840 / 139013\n",
      "progress:51968 / 139013\n",
      "progress:52096 / 139013\n",
      "progress:52224 / 139013\n",
      "progress:52352 / 139013\n",
      "progress:52480 / 139013\n",
      "progress:52608 / 139013\n",
      "progress:52736 / 139013\n",
      "progress:52864 / 139013\n",
      "progress:52992 / 139013\n",
      "progress:53120 / 139013\n",
      "progress:53248 / 139013\n",
      "progress:53376 / 139013\n",
      "progress:53504 / 139013\n",
      "progress:53632 / 139013\n",
      "progress:53760 / 139013\n",
      "progress:53888 / 139013\n",
      "progress:54016 / 139013\n",
      "progress:54144 / 139013\n",
      "progress:54272 / 139013\n",
      "progress:54400 / 139013\n",
      "progress:54528 / 139013\n",
      "progress:54656 / 139013\n",
      "progress:54784 / 139013\n",
      "progress:54912 / 139013\n",
      "progress:55040 / 139013\n",
      "progress:55168 / 139013\n",
      "progress:55296 / 139013\n",
      "progress:55424 / 139013\n",
      "progress:55552 / 139013\n",
      "progress:55680 / 139013\n",
      "progress:55808 / 139013\n",
      "progress:55936 / 139013\n",
      "progress:56064 / 139013\n",
      "progress:56192 / 139013\n",
      "progress:56320 / 139013\n",
      "progress:56448 / 139013\n",
      "progress:56576 / 139013\n",
      "progress:56704 / 139013\n",
      "progress:56832 / 139013\n",
      "progress:56960 / 139013\n",
      "progress:57088 / 139013\n",
      "progress:57216 / 139013\n",
      "progress:57344 / 139013\n",
      "progress:57472 / 139013\n",
      "progress:57600 / 139013\n",
      "progress:57728 / 139013\n",
      "progress:57856 / 139013\n",
      "progress:57984 / 139013\n",
      "progress:58112 / 139013\n",
      "progress:58240 / 139013\n",
      "progress:58368 / 139013\n",
      "progress:58496 / 139013\n",
      "progress:58624 / 139013\n",
      "progress:58752 / 139013\n",
      "progress:58880 / 139013\n",
      "progress:59008 / 139013\n",
      "progress:59136 / 139013\n",
      "progress:59264 / 139013\n",
      "progress:59392 / 139013\n",
      "progress:59520 / 139013\n",
      "progress:59648 / 139013\n",
      "progress:59776 / 139013\n",
      "progress:59904 / 139013\n",
      "progress:60032 / 139013\n",
      "progress:60160 / 139013\n",
      "progress:60288 / 139013\n",
      "progress:60416 / 139013\n",
      "progress:60544 / 139013\n",
      "progress:60672 / 139013\n",
      "progress:60800 / 139013\n",
      "progress:60928 / 139013\n",
      "progress:61056 / 139013\n",
      "progress:61184 / 139013\n",
      "progress:61312 / 139013\n",
      "progress:61440 / 139013\n",
      "progress:61568 / 139013\n",
      "progress:61696 / 139013\n",
      "progress:61824 / 139013\n",
      "progress:61952 / 139013\n",
      "progress:62080 / 139013\n",
      "progress:62208 / 139013\n",
      "progress:62336 / 139013\n",
      "progress:62464 / 139013\n",
      "progress:62592 / 139013\n",
      "progress:62720 / 139013\n",
      "progress:62848 / 139013\n",
      "progress:62976 / 139013\n",
      "progress:63104 / 139013\n",
      "progress:63232 / 139013\n",
      "progress:63360 / 139013\n",
      "progress:63488 / 139013\n",
      "progress:63616 / 139013\n",
      "progress:63744 / 139013\n",
      "progress:63872 / 139013\n",
      "progress:64000 / 139013\n",
      "progress:64128 / 139013\n",
      "progress:64256 / 139013\n",
      "progress:64384 / 139013\n",
      "progress:64512 / 139013\n",
      "progress:64640 / 139013\n",
      "progress:64768 / 139013\n",
      "progress:64896 / 139013\n",
      "progress:65024 / 139013\n",
      "progress:65152 / 139013\n",
      "progress:65280 / 139013\n",
      "progress:65408 / 139013\n",
      "progress:65536 / 139013\n",
      "progress:65664 / 139013\n",
      "progress:65792 / 139013\n",
      "progress:65920 / 139013\n",
      "progress:66048 / 139013\n",
      "progress:66176 / 139013\n",
      "progress:66304 / 139013\n",
      "progress:66432 / 139013\n",
      "progress:66560 / 139013\n",
      "progress:66688 / 139013\n",
      "progress:66816 / 139013\n",
      "progress:66944 / 139013\n",
      "progress:67072 / 139013\n",
      "progress:67200 / 139013\n",
      "progress:67328 / 139013\n",
      "progress:67456 / 139013\n",
      "progress:67584 / 139013\n",
      "progress:67712 / 139013\n",
      "progress:67840 / 139013\n",
      "progress:67968 / 139013\n",
      "progress:68096 / 139013\n",
      "progress:68224 / 139013\n",
      "progress:68352 / 139013\n",
      "progress:68480 / 139013\n",
      "progress:68608 / 139013\n",
      "progress:68736 / 139013\n",
      "progress:68864 / 139013\n",
      "progress:68992 / 139013\n",
      "progress:69120 / 139013\n",
      "progress:69248 / 139013\n",
      "progress:69376 / 139013\n",
      "progress:69504 / 139013\n",
      "progress:69632 / 139013\n",
      "progress:69760 / 139013\n",
      "progress:69888 / 139013\n",
      "progress:70016 / 139013\n",
      "progress:70144 / 139013\n",
      "progress:70272 / 139013\n",
      "progress:70400 / 139013\n",
      "progress:70528 / 139013\n",
      "progress:70656 / 139013\n",
      "progress:70784 / 139013\n",
      "progress:70912 / 139013\n",
      "progress:71040 / 139013\n",
      "progress:71168 / 139013\n",
      "progress:71296 / 139013\n",
      "progress:71424 / 139013\n",
      "progress:71552 / 139013\n",
      "progress:71680 / 139013\n",
      "progress:71808 / 139013\n",
      "progress:71936 / 139013\n",
      "progress:72064 / 139013\n",
      "progress:72192 / 139013\n",
      "progress:72320 / 139013\n",
      "progress:72448 / 139013\n",
      "progress:72576 / 139013\n",
      "progress:72704 / 139013\n",
      "progress:72832 / 139013\n",
      "progress:72960 / 139013\n",
      "progress:73088 / 139013\n",
      "progress:73216 / 139013\n",
      "progress:73344 / 139013\n",
      "progress:73472 / 139013\n",
      "progress:73600 / 139013\n",
      "progress:73728 / 139013\n",
      "progress:73856 / 139013\n",
      "progress:73984 / 139013\n",
      "progress:74112 / 139013\n",
      "progress:74240 / 139013\n",
      "progress:74368 / 139013\n",
      "progress:74496 / 139013\n",
      "progress:74624 / 139013\n",
      "progress:74752 / 139013\n",
      "progress:74880 / 139013\n",
      "progress:75008 / 139013\n",
      "progress:75136 / 139013\n",
      "progress:75264 / 139013\n",
      "progress:75392 / 139013\n",
      "progress:75520 / 139013\n",
      "progress:75648 / 139013\n",
      "progress:75776 / 139013\n",
      "progress:75904 / 139013\n",
      "progress:76032 / 139013\n",
      "progress:76160 / 139013\n",
      "progress:76288 / 139013\n",
      "progress:76416 / 139013\n",
      "progress:76544 / 139013\n",
      "progress:76672 / 139013\n",
      "progress:76800 / 139013\n",
      "progress:76928 / 139013\n",
      "progress:77056 / 139013\n",
      "progress:77184 / 139013\n",
      "progress:77312 / 139013\n",
      "progress:77440 / 139013\n",
      "progress:77568 / 139013\n",
      "progress:77696 / 139013\n",
      "progress:77824 / 139013\n",
      "progress:77952 / 139013\n",
      "progress:78080 / 139013\n",
      "progress:78208 / 139013\n",
      "progress:78336 / 139013\n",
      "progress:78464 / 139013\n",
      "progress:78592 / 139013\n",
      "progress:78720 / 139013\n",
      "progress:78848 / 139013\n",
      "progress:78976 / 139013\n",
      "progress:79104 / 139013\n",
      "progress:79232 / 139013\n",
      "progress:79360 / 139013\n",
      "progress:79488 / 139013\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m loss = criterion(output, tgt_output)\n\u001b[32m     34\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m optimizer.step()\n\u001b[32m     38\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhin\\OneDrive\\Desktop\\projects\\Not_done\\Transformer\\.venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhin\\OneDrive\\Desktop\\projects\\Not_done\\Transformer\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abhin\\OneDrive\\Desktop\\projects\\Not_done\\Transformer\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = 0.0001, betas = (0.9,0.98),eps = 1e-9)\n",
    "\n",
    "en_encoded = torch.tensor(en_encoded)\n",
    "sp_encoded = torch.tensor(sp_encoded)\n",
    "batch_size = 128\n",
    "\n",
    "num_samples = en_encoded.size(0)\n",
    "num_batches = num_samples // batch_size\n",
    "\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        src_batch = en_encoded[i:i+batch_size]\n",
    "        tgt_batch = sp_encoded[i:i+batch_size]\n",
    "        src_batch = src_batch.to(device)\n",
    "        tgt_batch = tgt_batch.to(device)\n",
    "        src_batch = src_batch.to(device)\n",
    "        tgt_batch = tgt_batch.to(device)\n",
    "\n",
    "        # Teacher forcing: decoder input is tgt[:, :-1]\n",
    "        tgt_input = tgt_batch[:, :-1]\n",
    "\n",
    "        # Model output\n",
    "        output = model(src_batch, tgt_input)   # (batch, seq, vocab_size)\n",
    "\n",
    "        # Target shifted by 1\n",
    "        tgt_output = tgt_batch[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        # Reshape output for loss\n",
    "        output = output.contiguous().view(-1, 10000)\n",
    "\n",
    "        loss = criterion(output, tgt_output)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        print(f\"progress:{i} / {num_samples}\")\n",
    "\n",
    "    print(f\"epoch {epoch+1}, loss: {total_loss/num_batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1180ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict,\"model.pth\")\n",
    "import pickle\n",
    "with open(\"en_encoder.pkl\",'wb') as f:\n",
    "    pickle.dump(en_encoder,f)\n",
    "with open(\"sp_encoder\",'wb') as f:\n",
    "    pickle.dump(sp_encoder,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}